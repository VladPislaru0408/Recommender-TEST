{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality-Focused Influencer Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2383 campaigns, 63932 influencers, 366352 interactions\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_PATH = '/home/vlad/Work/recommenders/Mydata/'\n",
    "campaigns = pd.read_csv(f'{DATA_PATH}Campaigns.csv')\n",
    "briefs = pd.read_csv(f'{DATA_PATH}Briefs.csv')\n",
    "influencers = pd.read_csv(f'{DATA_PATH}Influencers.csv')\n",
    "interactions = pd.read_csv(f'{DATA_PATH}Interactions.csv')\n",
    "print(f\"Loaded: {len(campaigns)} campaigns, {len(influencers)} influencers, {len(interactions)} interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality score distribution:\n",
      "count    63932.000000\n",
      "mean        22.627401\n",
      "std         12.603990\n",
      "min          0.000000\n",
      "25%         14.507045\n",
      "50%         21.758675\n",
      "75%         29.273299\n",
      "max         95.994725\n",
      "Name: quality_score, dtype: float64\n",
      "\n",
      "Top 10 quality influencers:\n",
      "                   account  followers  engagement  avg_likes  quality_score\n",
      "34297          marcelasmin    99338.0      367.05   362113.0      95.994725\n",
      "44618             maadaaaa    47523.0      282.80   134048.0      94.649420\n",
      "44987          dumpnitrish    32343.0      300.80    97091.8      93.947291\n",
      "28612  asawani_ngongomelon    25618.0      467.94   118832.0      93.521980\n",
      "47941           xoxo.grldn    21790.0      322.94    70262.8      93.226687\n",
      "30504  sean.marcus.leonida    20730.0      652.94   134476.0      93.135698\n",
      "47202         isisalencary    18504.0      427.48    78317.8      92.928440\n",
      "49276      urprettygalriri    15867.0     1928.44   305271.0      92.647928\n",
      "32635           sadidevera    13701.0      515.16    70260.0      92.380140\n",
      "20484          stefnedelcu    12691.0      919.41   116550.0      92.240429\n"
     ]
    }
   ],
   "source": [
    "# Calculate quality score for each influencer\n",
    "def calculate_quality_score(df):\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Engagement (40%)\n",
    "    eng_cap = result['engagement'].quantile(0.99)\n",
    "    result['eng_norm'] = (result['engagement'].clip(0, eng_cap) / eng_cap).fillna(0)\n",
    "    \n",
    "    # Followers (25%) - log scale\n",
    "    result['foll_log'] = np.log1p(result['followers'].fillna(0))\n",
    "    foll_max = result['foll_log'].quantile(0.99)\n",
    "    result['foll_norm'] = (result['foll_log'] / foll_max).clip(0, 1)\n",
    "    \n",
    "    # Avg likes (20%) - log scale\n",
    "    result['likes_log'] = np.log1p(result['avg_likes'].fillna(0))\n",
    "    likes_max = result['likes_log'].quantile(0.99)\n",
    "    result['likes_norm'] = (result['likes_log'] / likes_max).clip(0, 1)\n",
    "    \n",
    "    # Avg comments (15%) - log scale\n",
    "    result['comments_log'] = np.log1p(result['avg_comments'].fillna(0))\n",
    "    comments_max = result['comments_log'].quantile(0.99)\n",
    "    result['comments_norm'] = (result['comments_log'] / comments_max).clip(0, 1)\n",
    "    \n",
    "    # Weighted score (0-100)\n",
    "    result['quality_score'] = (\n",
    "        result['eng_norm'] * 40 +\n",
    "        result['foll_norm'] * 25 +\n",
    "        result['likes_norm'] * 20 +\n",
    "        result['comments_norm'] * 15\n",
    "    )\n",
    "    return result\n",
    "\n",
    "influencers_scored = calculate_quality_score(influencers)\n",
    "print(\"Quality score distribution:\")\n",
    "print(influencers_scored['quality_score'].describe())\n",
    "print(\"\\nTop 10 quality influencers:\")\n",
    "print(influencers_scored.nlargest(10, 'quality_score')[['account', 'followers', 'engagement', 'avg_likes', 'quality_score']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 63932, Quality: 20349, Filtered: 43583\n"
     ]
    }
   ],
   "source": [
    "# Filter to quality influencers only\n",
    "MIN_FOLLOWERS = 1000\n",
    "MIN_ENGAGEMENT = 0.5\n",
    "\n",
    "quality_influencers = influencers_scored[\n",
    "    (influencers_scored['followers'] >= MIN_FOLLOWERS) &\n",
    "    (influencers_scored['engagement'] >= MIN_ENGAGEMENT)\n",
    "].copy()\n",
    "\n",
    "print(f\"Total: {len(influencers_scored)}, Quality: {len(quality_influencers)}, Filtered: {len(influencers_scored) - len(quality_influencers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions with quality influencers: 239186\n"
     ]
    }
   ],
   "source": [
    "# Create accepted column and merge with quality influencers\n",
    "interactions['accepted'] = (interactions['status'] == 2).astype(int)\n",
    "\n",
    "df = interactions.merge(\n",
    "    quality_influencers[['id', 'network_id', 'followers', 'follows', 'engagement',\n",
    "                         'avg_likes', 'avg_comments', 'posts', 'reach', 'impressions',\n",
    "                         'tier_level', 'quality_score']],\n",
    "    left_on='creator_id', right_on='id', how='inner'\n",
    ")\n",
    "print(f\"Interactions with quality influencers: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging: 239186 rows\n"
     ]
    }
   ],
   "source": [
    "# Merge campaign data\n",
    "camp_cols = campaigns[['id', 'type_id', 'description', 'private', 'pre_approve']].copy()\n",
    "camp_cols = camp_cols.rename(columns={'id': 'camp_id', 'description': 'campaign_description'})\n",
    "df = df.merge(camp_cols, left_on='campaign_id', right_on='camp_id', how='left')\n",
    "\n",
    "# Merge brief data\n",
    "brief_cols = briefs[['id', 'description']].copy()\n",
    "brief_cols = brief_cols.rename(columns={'id': 'b_id', 'description': 'brief_description'})\n",
    "df = df.merge(brief_cols, left_on='brief_id', right_on='b_id', how='left')\n",
    "\n",
    "print(f\"After merging: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values filled\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values\n",
    "numeric_cols = ['followers', 'follows', 'engagement', 'avg_likes', 'avg_comments',\n",
    "                'posts', 'reach', 'impressions', 'quality_score']\n",
    "cat_cols = ['network_id', 'type_id', 'private', 'pre_approve', 'tier_level']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(-1).astype(int)\n",
    "print(\"Missing values filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features added\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF on unique briefs (memory efficient)\n",
    "df['brief_description'] = df['brief_description'].fillna('')\n",
    "df['campaign_description'] = df['campaign_description'].fillna('')\n",
    "\n",
    "unique_briefs = df[['brief_id', 'campaign_description', 'brief_description']].drop_duplicates('brief_id')\n",
    "unique_briefs['text'] = unique_briefs['campaign_description'] + ' ' + unique_briefs['brief_description']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(unique_briefs['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f'tfidf_{i}' for i in range(50)])\n",
    "tfidf_df['brief_id'] = unique_briefs['brief_id'].values\n",
    "\n",
    "df = df.merge(tfidf_df, on='brief_id', how='left')\n",
    "for i in range(50):\n",
    "    df[f'tfidf_{i}'] = df[f'tfidf_{i}'].fillna(0)\n",
    "print(\"TF-IDF features added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: (239186, 87)\n"
     ]
    }
   ],
   "source": [
    "# Historical stats\n",
    "creator_stats = interactions.groupby('creator_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "creator_stats.columns = ['creator_id', 'total_accepted', 'total_interactions', 'acceptance_rate']\n",
    "\n",
    "campaign_stats = interactions.groupby('campaign_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "campaign_stats.columns = ['campaign_id', 'camp_accepted', 'camp_interactions', 'camp_acceptance_rate']\n",
    "\n",
    "df = df.merge(creator_stats, on='creator_id', how='left')\n",
    "df = df.merge(campaign_stats, on='campaign_id', how='left')\n",
    "\n",
    "for col in ['acceptance_rate', 'total_accepted', 'total_interactions', 'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']:\n",
    "    df[col] = df[col].fillna(0)\n",
    "print(f\"Dataset ready: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 191348, Test: 47838\n"
     ]
    }
   ],
   "source": [
    "# Define features and train\n",
    "tfidf_feats = [f'tfidf_{i}' for i in range(50)]\n",
    "hist_feats = ['acceptance_rate', 'total_accepted', 'total_interactions',\n",
    "              'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']\n",
    "\n",
    "all_features = numeric_cols + cat_cols + tfidf_feats + hist_feats\n",
    "all_features = [f for f in all_features if f in df.columns]\n",
    "\n",
    "X = df[all_features].fillna(0)\n",
    "y = df['accepted']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.96337\n",
      "[200]\tvalid_0's auc: 0.964519\n",
      "[300]\tvalid_0's auc: 0.964936\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid_0's auc: 0.964936\n",
      "\n",
      "ROC-AUC: 0.9649\n",
      "\n",
      "=== NETWORK MAPPING ===\n",
      "Network 1: Instagram\n",
      "Network 8: TikTok\n",
      "Network 9: Other\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {'objective': 'binary', 'metric': 'auc', 'verbosity': -1,\n",
    "          'num_leaves': 31, 'learning_rate': 0.05, 'is_unbalance': True}\n",
    "\n",
    "model = lgb.train(params, train_data, 300, valid_sets=[test_data],\n",
    "                  callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"\\nROC-AUC: {roc_auc_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Show network mapping\n",
    "print(\"\\n=== NETWORK MAPPING ===\")\n",
    "print(\"Network 1: Instagram\")\n",
    "print(\"Network 8: TikTok\")\n",
    "print(\"Network 9: Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Campaign-Aware Recommendation Functions\n",
    "\n",
    "Key insight: Different campaigns target different networks (Instagram vs TikTok) and have zero overlap in successful influencers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_campaign(campaign_id, brief_id, top_n=10,\n",
    "                           min_followers=1000, min_engagement=1.0,\n",
    "                           quality_weight=0.7, acceptance_weight=0.3):\n",
    "    \"\"\"\n",
    "    Campaign-aware recommendations that:\n",
    "    1. Filter by campaign's network (Instagram/TikTok)\n",
    "    2. Use text similarity between influencer bio and campaign description\n",
    "    3. Weight quality + acceptance probability\n",
    "    \"\"\"\n",
    "    camp_info = campaigns[campaigns['id'] == campaign_id].iloc[0]\n",
    "    brief_info = briefs[briefs['id'] == brief_id].iloc[0]\n",
    "    \n",
    "    campaign_network = camp_info['network_id']\n",
    "    campaign_text = str(camp_info.get('description', '')) + ' ' + str(brief_info.get('description', ''))\n",
    "    \n",
    "    print(f\"Campaign: {camp_info['name']}\")\n",
    "    print(f\"Network: {campaign_network} ({'Instagram' if campaign_network == 1 else 'TikTok' if campaign_network == 8 else 'Other'})\") \n",
    "    \n",
    "    # Start with quality influencers\n",
    "    pred_df = influencers_scored[\n",
    "        (influencers_scored['followers'] >= min_followers) &\n",
    "        (influencers_scored['engagement'] >= min_engagement)\n",
    "    ].copy()\n",
    "    \n",
    "    # IMPORTANT: Filter by campaign network!\n",
    "    pred_df = pred_df[pred_df['network_id'] == campaign_network]\n",
    "    print(f\"Influencers on this network: {len(pred_df)}\")\n",
    "    \n",
    "    # Exclude already interacted\n",
    "    already = interactions[interactions['campaign_id'] == campaign_id]['creator_id'].unique()\n",
    "    pred_df = pred_df[~pred_df['id'].isin(already)]\n",
    "    print(f\"After excluding already contacted: {len(pred_df)}\")\n",
    "    \n",
    "    if len(pred_df) == 0:\n",
    "        print(\"No influencers available!\")\n",
    "        return None\n",
    "    \n",
    "    # Add campaign features for model\n",
    "    pred_df['campaign_id'] = campaign_id\n",
    "    pred_df['type_id'] = camp_info.get('type_id', -1)\n",
    "    pred_df['private'] = camp_info.get('private', 0)\n",
    "    pred_df['pre_approve'] = camp_info.get('pre_approve', 0)\n",
    "    \n",
    "    # TF-IDF features from campaign text\n",
    "    tfidf_arr = tfidf.transform([campaign_text]).toarray()[0]\n",
    "    for i in range(50):\n",
    "        pred_df[f'tfidf_{i}'] = tfidf_arr[i]\n",
    "    \n",
    "    # Calculate text similarity between influencer bio and campaign\n",
    "    pred_df['biography'] = pred_df['biography'].fillna('')\n",
    "    bio_tfidf = tfidf.transform(pred_df['biography'].tolist())\n",
    "    campaign_tfidf = tfidf.transform([campaign_text])\n",
    "    \n",
    "    # Cosine similarity\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(bio_tfidf, campaign_tfidf).flatten()\n",
    "    pred_df['bio_similarity'] = similarities\n",
    "    \n",
    "    # Add historical features\n",
    "    pred_df = pred_df.merge(creator_stats, left_on='id', right_on='creator_id', how='left')\n",
    "    pred_df = pred_df.merge(campaign_stats, on='campaign_id', how='left')\n",
    "    \n",
    "    # Fill missing features\n",
    "    for col in all_features:\n",
    "        if col not in pred_df.columns:\n",
    "            pred_df[col] = 0\n",
    "        pred_df[col] = pred_df[col].fillna(0)\n",
    "    \n",
    "    # Predict acceptance probability\n",
    "    pred_df['acceptance_prob'] = model.predict(pred_df[all_features])\n",
    "    \n",
    "    # Normalize scores\n",
    "    pred_df['quality_norm'] = pred_df['quality_score'] / pred_df['quality_score'].max()\n",
    "    pred_df['similarity_norm'] = pred_df['bio_similarity'] / (pred_df['bio_similarity'].max() + 0.001)\n",
    "    \n",
    "    # Combined score: quality + acceptance + bio similarity\n",
    "    pred_df['final_score'] = (\n",
    "        pred_df['quality_norm'] * quality_weight * 0.6 +\n",
    "        pred_df['acceptance_prob'] * acceptance_weight +\n",
    "        pred_df['similarity_norm'] * quality_weight * 0.4  # Bio match is part of quality\n",
    "    )\n",
    "    \n",
    "    result = pred_df.nlargest(top_n, 'final_score')[\n",
    "        ['id', 'account', 'name', 'followers', 'engagement', 'avg_likes', \n",
    "         'quality_score', 'bio_similarity', 'acceptance_prob', 'final_score']\n",
    "    ].copy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old function - we'll use the new campaign-aware one\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Evaluation: Train on Old Campaigns, Test on New\n",
    "\n",
    "We'll:\n",
    "1. Train on campaigns before April 2024\n",
    "2. Test on campaigns after April 2024\n",
    "3. For each test campaign, recommend top N influencers\n",
    "4. Measure how many of our recommendations match the actual accepted influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training campaigns: 1029\n",
      "Test campaigns: 1354\n",
      "Training interactions: 139192\n",
      "Test interactions: 227160\n",
      "\n",
      "Test campaigns with 5+ accepted influencers: 677\n"
     ]
    }
   ],
   "source": [
    "# Split campaigns by date\n",
    "campaigns['created_at'] = pd.to_datetime(campaigns['created_at'])\n",
    "SPLIT_DATE = '2024-04-01'\n",
    "\n",
    "train_campaigns = campaigns[campaigns['created_at'] < SPLIT_DATE]['id'].tolist()\n",
    "test_campaigns = campaigns[campaigns['created_at'] >= SPLIT_DATE]['id'].tolist()\n",
    "\n",
    "# Get interactions for train/test\n",
    "train_interactions = interactions[interactions['campaign_id'].isin(train_campaigns)].copy()\n",
    "test_interactions = interactions[interactions['campaign_id'].isin(test_campaigns)].copy()\n",
    "\n",
    "print(f\"Training campaigns: {len(train_campaigns)}\")\n",
    "print(f\"Test campaigns: {len(test_campaigns)}\")\n",
    "print(f\"Training interactions: {len(train_interactions)}\")\n",
    "print(f\"Test interactions: {len(test_interactions)}\")\n",
    "\n",
    "# Filter test to campaigns with at least 5 accepted\n",
    "test_accepted = test_interactions[test_interactions['accepted'] == 1].groupby('campaign_id').size()\n",
    "test_camps_with_accepted = test_accepted[test_accepted >= 5].index.tolist()\n",
    "print(f\"\\nTest campaigns with 5+ accepted influencers: {len(test_camps_with_accepted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 103565 rows\n"
     ]
    }
   ],
   "source": [
    "# Build training data using ONLY training campaigns\n",
    "# Historical stats from training data only\n",
    "train_creator_stats = train_interactions.groupby('creator_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "train_creator_stats.columns = ['creator_id', 'total_accepted', 'total_interactions', 'acceptance_rate']\n",
    "\n",
    "train_campaign_stats = train_interactions.groupby('campaign_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "train_campaign_stats.columns = ['campaign_id', 'camp_accepted', 'camp_interactions', 'camp_acceptance_rate']\n",
    "\n",
    "# Build training dataset\n",
    "train_df = train_interactions.merge(\n",
    "    quality_influencers[['id', 'network_id', 'followers', 'follows', 'engagement',\n",
    "                         'avg_likes', 'avg_comments', 'posts', 'reach', 'impressions',\n",
    "                         'tier_level', 'quality_score', 'biography']],\n",
    "    left_on='creator_id', right_on='id', how='inner'\n",
    ")\n",
    "\n",
    "# Merge campaign data\n",
    "camp_cols = campaigns[['id', 'type_id', 'description', 'private', 'pre_approve', 'network_id']].copy()\n",
    "camp_cols = camp_cols.rename(columns={'id': 'camp_id', 'description': 'campaign_description', 'network_id': 'camp_network'})\n",
    "train_df = train_df.merge(camp_cols, left_on='campaign_id', right_on='camp_id', how='left')\n",
    "\n",
    "# Merge brief data\n",
    "brief_cols = briefs[['id', 'description']].copy()\n",
    "brief_cols = brief_cols.rename(columns={'id': 'b_id', 'description': 'brief_description'})\n",
    "train_df = train_df.merge(brief_cols, left_on='brief_id', right_on='b_id', how='left')\n",
    "\n",
    "print(f\"Training data: {len(train_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features prepared: (103565, 89)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for training\n",
    "train_df['brief_description'] = train_df['brief_description'].fillna('')\n",
    "train_df['campaign_description'] = train_df['campaign_description'].fillna('')\n",
    "\n",
    "# Fill numeric/categorical\n",
    "for col in numeric_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].fillna(0)\n",
    "for col in cat_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].fillna(-1).astype(int)\n",
    "\n",
    "# TF-IDF on training briefs only\n",
    "train_unique_briefs = train_df[['brief_id', 'campaign_description', 'brief_description']].drop_duplicates('brief_id')\n",
    "train_unique_briefs['text'] = train_unique_briefs['campaign_description'] + ' ' + train_unique_briefs['brief_description']\n",
    "\n",
    "train_tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "train_tfidf_matrix = train_tfidf.fit_transform(train_unique_briefs['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(train_tfidf_matrix.toarray(), columns=[f'tfidf_{i}' for i in range(50)])\n",
    "tfidf_df['brief_id'] = train_unique_briefs['brief_id'].values\n",
    "\n",
    "train_df = train_df.merge(tfidf_df, on='brief_id', how='left')\n",
    "for i in range(50):\n",
    "    train_df[f'tfidf_{i}'] = train_df[f'tfidf_{i}'].fillna(0)\n",
    "\n",
    "# Add historical stats\n",
    "train_df = train_df.merge(train_creator_stats, on='creator_id', how='left')\n",
    "train_df = train_df.merge(train_campaign_stats, on='campaign_id', how='left')\n",
    "for col in ['acceptance_rate', 'total_accepted', 'total_interactions', 'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']:\n",
    "    train_df[col] = train_df[col].fillna(0)\n",
    "\n",
    "print(f\"Training features prepared: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 103565 samples with 70 features\n",
      "Model trained on historical data only!\n"
     ]
    }
   ],
   "source": [
    "# Train model on training data only\n",
    "tfidf_feats = [f'tfidf_{i}' for i in range(50)]\n",
    "hist_feats = ['acceptance_rate', 'total_accepted', 'total_interactions',\n",
    "              'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']\n",
    "\n",
    "train_features = numeric_cols + cat_cols + tfidf_feats + hist_feats\n",
    "train_features = [f for f in train_features if f in train_df.columns]\n",
    "\n",
    "X_train = train_df[train_features].fillna(0)\n",
    "y_train = train_df['accepted']\n",
    "\n",
    "print(f\"Training on {len(X_train)} samples with {len(train_features)} features\")\n",
    "\n",
    "# Train\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {'objective': 'binary', 'metric': 'auc', 'verbosity': -1,\n",
    "          'num_leaves': 31, 'learning_rate': 0.05, 'is_unbalance': True}\n",
    "\n",
    "eval_model = lgb.train(params, train_data, 200)\n",
    "print(\"Model trained on historical data only!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: For a test campaign, recommend top N and check overlap with actual accepted\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_recommendations_for_eval(campaign_id, brief_id, top_n=50):\n",
    "    \"\"\"Get top N recommendations for a campaign using the trained model.\"\"\"\n",
    "    camp_info = campaigns[campaigns['id'] == campaign_id].iloc[0]\n",
    "    brief_info = briefs[briefs['id'] == brief_id]\n",
    "    if len(brief_info) == 0:\n",
    "        return None\n",
    "    brief_info = brief_info.iloc[0]\n",
    "    \n",
    "    campaign_network = camp_info['network_id']\n",
    "    campaign_text = str(camp_info.get('description', '')) + ' ' + str(brief_info.get('description', ''))\n",
    "    \n",
    "    # Get quality influencers on same network\n",
    "    pred_df = influencers_scored[\n",
    "        (influencers_scored['followers'] >= 1000) &\n",
    "        (influencers_scored['engagement'] >= 0.5) &\n",
    "        (influencers_scored['network_id'] == campaign_network)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(pred_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Add features\n",
    "    pred_df['campaign_id'] = campaign_id\n",
    "    pred_df['type_id'] = camp_info.get('type_id', -1)\n",
    "    pred_df['private'] = camp_info.get('private', 0)\n",
    "    pred_df['pre_approve'] = camp_info.get('pre_approve', 0)\n",
    "    \n",
    "    # TF-IDF\n",
    "    try:\n",
    "        tfidf_arr = train_tfidf.transform([campaign_text]).toarray()[0]\n",
    "        for i in range(50):\n",
    "            pred_df[f'tfidf_{i}'] = tfidf_arr[i]\n",
    "    except:\n",
    "        for i in range(50):\n",
    "            pred_df[f'tfidf_{i}'] = 0\n",
    "    \n",
    "    # Historical stats (from training data only!)\n",
    "    pred_df = pred_df.merge(train_creator_stats, left_on='id', right_on='creator_id', how='left')\n",
    "    pred_df = pred_df.merge(train_campaign_stats, on='campaign_id', how='left')\n",
    "    \n",
    "    # Fill missing\n",
    "    for col in train_features:\n",
    "        if col not in pred_df.columns:\n",
    "            pred_df[col] = 0\n",
    "        pred_df[col] = pred_df[col].fillna(0)\n",
    "    \n",
    "    # Predict\n",
    "    pred_df['score'] = eval_model.predict(pred_df[train_features])\n",
    "    \n",
    "    # Add quality score to final ranking\n",
    "    pred_df['quality_norm'] = pred_df['quality_score'] / pred_df['quality_score'].max()\n",
    "    pred_df['final_score'] = pred_df['score'] * 0.5 + pred_df['quality_norm'] * 0.5\n",
    "    \n",
    "    return pred_df.nlargest(top_n, 'final_score')['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 99 test campaigns\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test campaigns\n",
    "results = []\n",
    "\n",
    "for camp_id in test_camps_with_accepted[:100]:  # Test on first 100 campaigns\n",
    "    # Get actual accepted influencers\n",
    "    actual_accepted = test_interactions[\n",
    "        (test_interactions['campaign_id'] == camp_id) & \n",
    "        (test_interactions['accepted'] == 1)\n",
    "    ]['creator_id'].unique()\n",
    "    \n",
    "    if len(actual_accepted) < 5:\n",
    "        continue\n",
    "    \n",
    "    # Get brief\n",
    "    camp_briefs = briefs[briefs['campaign_id'] == camp_id]\n",
    "    if len(camp_briefs) == 0:\n",
    "        continue\n",
    "    brief_id = camp_briefs.iloc[0]['id']\n",
    "    \n",
    "    # Get recommendations\n",
    "    for top_n in [10, 25, 50, 100]:\n",
    "        recommended = get_recommendations_for_eval(camp_id, brief_id, top_n=top_n)\n",
    "        if recommended is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        recommended_set = set(recommended)\n",
    "        actual_set = set(actual_accepted)\n",
    "        \n",
    "        hits = len(recommended_set & actual_set)\n",
    "        precision = hits / len(recommended_set) if len(recommended_set) > 0 else 0\n",
    "        recall = hits / len(actual_set) if len(actual_set) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'campaign_id': camp_id,\n",
    "            'top_n': top_n,\n",
    "            'actual_accepted': len(actual_set),\n",
    "            'recommended': len(recommended_set),\n",
    "            'hits': hits,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Evaluated {results_df['campaign_id'].nunique()} test campaigns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATION RESULTS: Model trained on old campaigns, tested on new\n",
      "======================================================================\n",
      "\n",
      "=== SUMMARY BY TOP_N ===\n",
      "       num_campaigns  hits  actual_accepted  overall_precision  overall_recall\n",
      "top_n                                                                         \n",
      "10                99     0             2024           0.000000        0.000000\n",
      "25                99     1             2024           0.000404        0.000494\n",
      "50                99     1             2024           0.000202        0.000494\n",
      "100               99     2             2024           0.000202        0.000988\n",
      "\n",
      "\n",
      "=== INTERPRETATION ===\n",
      "Top 10: 0.0% of actual accepted influencers found in recommendations\n",
      "Top 25: 0.0% of actual accepted influencers found in recommendations\n",
      "Top 50: 0.0% of actual accepted influencers found in recommendations\n",
      "Top 100: 0.1% of actual accepted influencers found in recommendations\n"
     ]
    }
   ],
   "source": [
    "# Show evaluation results\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION RESULTS: Model trained on old campaigns, tested on new\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate by top_n\n",
    "summary = results_df.groupby('top_n').agg({\n",
    "    'hits': 'sum',\n",
    "    'actual_accepted': 'sum',\n",
    "    'recommended': 'sum',\n",
    "    'precision': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'campaign_id': 'count'\n",
    "}).rename(columns={'campaign_id': 'num_campaigns'})\n",
    "\n",
    "summary['overall_precision'] = summary['hits'] / summary['recommended']\n",
    "summary['overall_recall'] = summary['hits'] / summary['actual_accepted']\n",
    "\n",
    "print(\"\\n=== SUMMARY BY TOP_N ===\")\n",
    "print(summary[['num_campaigns', 'hits', 'actual_accepted', 'overall_precision', 'overall_recall']].to_string())\n",
    "\n",
    "print(\"\\n\\n=== INTERPRETATION ===\")\n",
    "for top_n in [10, 25, 50, 100]:\n",
    "    row = summary.loc[top_n]\n",
    "    print(f\"Top {top_n}: {row['overall_recall']*100:.1f}% of actual accepted influencers found in recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1602957946.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    sample_camp = test_camps_with_accepted[0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Cell 26: Diagnose why recommendations don't match\n",
    "sample_camp = test_camps_with_accepted[0]\n",
    "camp_briefs = briefs[briefs['campaign_id'] == sample_camp]\n",
    "brief_id = camp_briefs.iloc[0]['id']\n",
    "\n",
    "# Who actually accepted\n",
    "actual = test_interactions[\n",
    "  (test_interactions['campaign_id'] == sample_camp) &\n",
    "  (test_interactions['accepted'] == 1)\n",
    "]['creator_id'].unique()\n",
    "\n",
    "print(f\"Campaign {sample_camp}\")\n",
    "print(f\"Actual accepted: {len(actual)}\")\n",
    "\n",
    "# Check if actual accepted are in quality_influencers\n",
    "actual_in_quality = influencers_scored[influencers_scored['id'].isin(actual)]\n",
    "print(f\"Accepted influencers that meet quality threshold: {len(actual_in_quality)}\")\n",
    "\n",
    "# Check their quality scores\n",
    "if len(actual_in_quality) > 0:\n",
    "  print(f\"\\nActual accepted influencers stats:\")\n",
    "  print(f\"  Followers range: {actual_in_quality['followers'].min():.0f} - {actual_in_quality['followers'].max():.0f}\")\n",
    "  print(f\"  Engagement range: {actual_in_quality['engagement'].min():.2f} - {actual_in_quality['engagement'].max():.2f}\")\n",
    "  print(f\"  Quality score range: {actual_in_quality['quality_score'].min():.1f} - {actual_in_quality['quality_score'].max():.1f}\")\n",
    "\n",
    "# How many pass our filters?\n",
    "filtered = actual_in_quality[\n",
    "  (actual_in_quality['followers'] >= 1000) &\n",
    "  (actual_in_quality['engagement'] >= 0.5)\n",
    "]\n",
    "print(f\"  Pass our filters (1000+ followers, 0.5+ engagement): {len(filtered)}\")\n",
    "\n",
    "# Network check\n",
    "camp_network = campaigns[campaigns['id'] == sample_camp]['network_id'].values[0]\n",
    "print(f\"\\nCampaign network: {camp_network}\")\n",
    "print(f\"Accepted influencers on this network: {len(actual_in_quality[actual_in_quality['network_id'] == camp_network])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
