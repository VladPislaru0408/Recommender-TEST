{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality-Focused Influencer Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2383 campaigns, 63932 influencers, 366352 interactions\n",
      "Groups: 2614, User_Info: 53433, User_Categories: 133947\n"
     ]
    }
   ],
   "source": [
    "# Load ALL data including group filters\n",
    "DATA_PATH = '/home/vlad/Work/fj-recommendations/Mydata/data/'\n",
    "\n",
    "# Core data\n",
    "campaigns = pd.read_csv(f'{DATA_PATH}Campaigns.csv')\n",
    "briefs = pd.read_csv(f'{DATA_PATH}Briefs.csv')\n",
    "influencers = pd.read_csv(f'{DATA_PATH}Influencers.csv')\n",
    "interactions = pd.read_csv(f'{DATA_PATH}Interactions.csv')\n",
    "\n",
    "# Group filter data\n",
    "groups = pd.read_csv(f'{DATA_PATH}Groups.csv')\n",
    "user_info = pd.read_csv(f'{DATA_PATH}User_Info.csv')\n",
    "group_locations = pd.read_csv(f'{DATA_PATH}Group_Locations.csv')\n",
    "group_categories = pd.read_csv(f'{DATA_PATH}Group_Categories.csv')\n",
    "group_creator_prefs = pd.read_csv(f'{DATA_PATH}Group_Creator_Preferences.csv')\n",
    "creator_prefs = pd.read_csv(f'{DATA_PATH}Creator_Preferences.csv')\n",
    "user_categories = pd.read_csv(f'{DATA_PATH}User_Categories.csv')\n",
    "\n",
    "print(f\"Loaded: {len(campaigns)} campaigns, {len(influencers)} influencers, {len(interactions)} interactions\")\n",
    "print(f\"Groups: {len(groups)}, User_Info: {len(user_info)}, User_Categories: {len(user_categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality score calculated\n",
      "Influencers with user_id: 38101\n"
     ]
    }
   ],
   "source": [
    "# Calculate quality score for each influencer\n",
    "def calculate_quality_score(df):\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Engagement (40%)\n",
    "    eng_cap = result['engagement'].quantile(0.99)\n",
    "    result['eng_norm'] = (result['engagement'].clip(0, eng_cap) / eng_cap).fillna(0)\n",
    "    \n",
    "    # Followers (25%) - log scale\n",
    "    result['foll_log'] = np.log1p(result['followers'].fillna(0))\n",
    "    foll_max = result['foll_log'].quantile(0.99)\n",
    "    result['foll_norm'] = (result['foll_log'] / foll_max).clip(0, 1)\n",
    "    \n",
    "    # Avg likes (20%) - log scale\n",
    "    result['likes_log'] = np.log1p(result['avg_likes'].fillna(0))\n",
    "    likes_max = result['likes_log'].quantile(0.99)\n",
    "    result['likes_norm'] = (result['likes_log'] / likes_max).clip(0, 1)\n",
    "    \n",
    "    # Avg comments (15%) - log scale\n",
    "    result['comments_log'] = np.log1p(result['avg_comments'].fillna(0))\n",
    "    comments_max = result['comments_log'].quantile(0.99)\n",
    "    result['comments_norm'] = (result['comments_log'] / comments_max).clip(0, 1)\n",
    "    \n",
    "    # Weighted score (0-100)\n",
    "    result['quality_score'] = (\n",
    "        result['eng_norm'] * 40 +\n",
    "        result['foll_norm'] * 25 +\n",
    "        result['likes_norm'] * 20 +\n",
    "        result['comments_norm'] * 15\n",
    "    )\n",
    "    return result\n",
    "\n",
    "influencers_scored = calculate_quality_score(influencers)\n",
    "print(\"Quality score calculated\")\n",
    "print(f\"Influencers with user_id: {influencers_scored['user_id'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_eligible_influencers defined. Test campaign 2817: 7476 eligible\n"
     ]
    }
   ],
   "source": [
    "# Define group filter function\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_eligible_influencers(campaign_id):\n",
    "    \"\"\"\n",
    "    Get influencers that pass all group filters for a campaign.\n",
    "    Returns list of eligible influencer IDs.\n",
    "    \"\"\"\n",
    "    camp_groups = groups[groups['campaign_id'] == campaign_id]\n",
    "    \n",
    "    if len(camp_groups) == 0:\n",
    "        return influencers_scored['id'].tolist()\n",
    "    \n",
    "    eligible_ids = set()\n",
    "    \n",
    "    for _, group in camp_groups.iterrows():\n",
    "        group_id = group['id']\n",
    "        candidates = influencers_scored[influencers_scored['user_id'].notna()].copy()\n",
    "        \n",
    "        # 1. TIER FILTER\n",
    "        if pd.notna(group['creators_tiers']):\n",
    "            try:\n",
    "                tiers = json.loads(group['creators_tiers'])\n",
    "                candidates = candidates[candidates['tier_level'].isin(tiers)]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 2. COUNTRY FILTER\n",
    "        if pd.notna(group['country_id']):\n",
    "            country_users = set(user_info[user_info['country_id'] == group['country_id']]['user_id'])\n",
    "            candidates = candidates[candidates['user_id'].isin(country_users)]\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 3. LOCATION FILTER - Skip if region is NaN\n",
    "        grp_locations = group_locations[group_locations['group_id'] == group_id]\n",
    "        if len(grp_locations) > 0:\n",
    "            valid_locations = grp_locations[grp_locations['region_id'].notna()]\n",
    "            if len(valid_locations) > 0:\n",
    "                location_users = set()\n",
    "                for _, loc in valid_locations.iterrows():\n",
    "                    if pd.isna(loc['city_id']):\n",
    "                        region_users = user_info[user_info['region_id'] == loc['region_id']]['user_id']\n",
    "                    else:\n",
    "                        region_users = user_info[\n",
    "                            (user_info['region_id'] == loc['region_id']) & \n",
    "                            (user_info['city_id'] == loc['city_id'])\n",
    "                        ]['user_id']\n",
    "                    location_users.update(region_users)\n",
    "                candidates = candidates[candidates['user_id'].isin(location_users)]\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 4. CATEGORY FILTER\n",
    "        grp_cats = group_categories[group_categories['group_id'] == group_id]['category_id'].tolist()\n",
    "        if len(grp_cats) > 0:\n",
    "            cat_users = set(user_categories[user_categories['category_id'].isin(grp_cats)]['user_id'])\n",
    "            candidates = candidates[candidates['user_id'].isin(cat_users)]\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 5. GENDER FILTER\n",
    "        grp_prefs = group_creator_prefs[group_creator_prefs['group_id'] == group_id]\n",
    "        gender_pref_ids = grp_prefs['creator_preference_id'].tolist()\n",
    "        gender_prefs = creator_prefs[\n",
    "            (creator_prefs['id'].isin(gender_pref_ids)) & \n",
    "            (creator_prefs['creator_preference_type_id'] == 2)\n",
    "        ]['name'].tolist()\n",
    "        \n",
    "        if len(gender_prefs) > 0:\n",
    "            gender_map = {'Women': 2, 'Men': 1, 'Other': 0}\n",
    "            gender_values = [gender_map.get(g) for g in gender_prefs if g in gender_map]\n",
    "            if gender_values:\n",
    "                gender_users = set(user_info[user_info['gender'].isin(gender_values)]['user_id'])\n",
    "                candidates = candidates[candidates['user_id'].isin(gender_users)]\n",
    "        \n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 6. AGE FILTER\n",
    "        age_pref_ids = grp_prefs['creator_preference_id'].tolist()\n",
    "        age_prefs = creator_prefs[\n",
    "            (creator_prefs['id'].isin(age_pref_ids)) & \n",
    "            (creator_prefs['creator_preference_type_id'] == 1)\n",
    "        ]['name'].tolist()\n",
    "        \n",
    "        if len(age_prefs) > 0:\n",
    "            today = datetime.now()\n",
    "            age_users = set()\n",
    "            user_info_with_age = user_info[user_info['birthday'].notna()].copy()\n",
    "            user_info_with_age['birthday'] = pd.to_datetime(user_info_with_age['birthday'], errors='coerce')\n",
    "            user_info_with_age['age'] = (today - user_info_with_age['birthday']).dt.days // 365\n",
    "            \n",
    "            for age_range in age_prefs:\n",
    "                if age_range == '55+':\n",
    "                    matching = user_info_with_age[user_info_with_age['age'] >= 55]['user_id']\n",
    "                elif '-' in age_range:\n",
    "                    min_age, max_age = map(int, age_range.split('-'))\n",
    "                    matching = user_info_with_age[\n",
    "                        (user_info_with_age['age'] >= min_age) & \n",
    "                        (user_info_with_age['age'] <= max_age)\n",
    "                    ]['user_id']\n",
    "                else:\n",
    "                    continue\n",
    "                age_users.update(matching)\n",
    "            \n",
    "            if age_users:\n",
    "                candidates = candidates[candidates['user_id'].isin(age_users)]\n",
    "        \n",
    "        eligible_ids.update(candidates['id'].tolist())\n",
    "    \n",
    "    return list(eligible_ids)\n",
    "\n",
    "# Test it\n",
    "eligible = get_eligible_influencers(2817)\n",
    "print(f\"get_eligible_influencers defined. Test campaign 2817: {len(eligible)} eligible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions with influencers: 366352\n"
     ]
    }
   ],
   "source": [
    "# Create accepted column and merge with influencers\n",
    "interactions['accepted'] = (interactions['status'] == 2).astype(int)\n",
    "\n",
    "df = interactions.merge(\n",
    "    influencers_scored[['id', 'network_id', 'followers', 'follows', 'engagement',\n",
    "                         'avg_likes', 'avg_comments', 'posts', 'reach', 'impressions',\n",
    "                         'tier_level', 'quality_score']],\n",
    "    left_on='creator_id', right_on='id', how='inner'\n",
    ")\n",
    "print(f\"Interactions with influencers: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging: 366352 rows\n"
     ]
    }
   ],
   "source": [
    "# Merge campaign data\n",
    "camp_cols = campaigns[['id', 'type_id', 'description', 'private', 'pre_approve']].copy()\n",
    "camp_cols = camp_cols.rename(columns={'id': 'camp_id', 'description': 'campaign_description'})\n",
    "df = df.merge(camp_cols, left_on='campaign_id', right_on='camp_id', how='left')\n",
    "\n",
    "# Merge brief data\n",
    "brief_cols = briefs[['id', 'description']].copy()\n",
    "brief_cols = brief_cols.rename(columns={'id': 'b_id', 'description': 'brief_description'})\n",
    "df = df.merge(brief_cols, left_on='brief_id', right_on='b_id', how='left')\n",
    "\n",
    "print(f\"After merging: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values filled\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values\n",
    "numeric_cols = ['followers', 'follows', 'engagement', 'avg_likes', 'avg_comments',\n",
    "                'posts', 'reach', 'impressions', 'quality_score']\n",
    "cat_cols = ['network_id', 'type_id', 'private', 'pre_approve', 'tier_level']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(-1).astype(int)\n",
    "print(\"Missing values filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features added\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF on unique briefs (memory efficient)\n",
    "df['brief_description'] = df['brief_description'].fillna('')\n",
    "df['campaign_description'] = df['campaign_description'].fillna('')\n",
    "\n",
    "unique_briefs = df[['brief_id', 'campaign_description', 'brief_description']].drop_duplicates('brief_id')\n",
    "unique_briefs['text'] = unique_briefs['campaign_description'] + ' ' + unique_briefs['brief_description']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(unique_briefs['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f'tfidf_{i}' for i in range(50)])\n",
    "tfidf_df['brief_id'] = unique_briefs['brief_id'].values\n",
    "\n",
    "df = df.merge(tfidf_df, on='brief_id', how='left')\n",
    "for i in range(50):\n",
    "    df[f'tfidf_{i}'] = df[f'tfidf_{i}'].fillna(0)\n",
    "print(\"TF-IDF features added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: (366352, 87)\n"
     ]
    }
   ],
   "source": [
    "# Historical stats\n",
    "creator_stats = interactions.groupby('creator_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "creator_stats.columns = ['creator_id', 'total_accepted', 'total_interactions', 'acceptance_rate']\n",
    "\n",
    "campaign_stats = interactions.groupby('campaign_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "campaign_stats.columns = ['campaign_id', 'camp_accepted', 'camp_interactions', 'camp_acceptance_rate']\n",
    "\n",
    "df = df.merge(creator_stats, on='creator_id', how='left')\n",
    "df = df.merge(campaign_stats, on='campaign_id', how='left')\n",
    "\n",
    "for col in ['acceptance_rate', 'total_accepted', 'total_interactions', 'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']:\n",
    "    df[col] = df[col].fillna(0)\n",
    "print(f\"Dataset ready: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 293081, Test: 73271\n"
     ]
    }
   ],
   "source": [
    "# Define features and train\n",
    "tfidf_feats = [f'tfidf_{i}' for i in range(50)]\n",
    "hist_feats = ['acceptance_rate', 'total_accepted', 'total_interactions',\n",
    "              'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']\n",
    "\n",
    "all_features = numeric_cols + cat_cols + tfidf_feats + hist_feats\n",
    "all_features = [f for f in all_features if f in df.columns]\n",
    "\n",
    "X = df[all_features].fillna(0)\n",
    "y = df['accepted']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Campaign-Aware Recommendation Functions\n",
    "\n",
    "Key insight: Different campaigns target different networks (Instagram vs TikTok) and have zero overlap in successful influencers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Evaluation: Train on Old Campaigns, Test on New\n",
    "\n",
    "We'll:\n",
    "1. Train on campaigns before April 2024\n",
    "2. Test on campaigns after April 2024\n",
    "3. For each test campaign, recommend top N influencers\n",
    "4. Measure how many of our recommendations match the actual accepted influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training campaigns: 1029\n",
      "Test campaigns: 1354\n",
      "Training interactions: 139192\n",
      "Test interactions: 227160\n",
      "\n",
      "Test campaigns with 5+ accepted influencers: 677\n"
     ]
    }
   ],
   "source": [
    "# Split campaigns by date\n",
    "campaigns['created_at'] = pd.to_datetime(campaigns['created_at'])\n",
    "SPLIT_DATE = '2024-04-01'\n",
    "\n",
    "train_campaigns = campaigns[campaigns['created_at'] < SPLIT_DATE]['id'].tolist()\n",
    "test_campaigns = campaigns[campaigns['created_at'] >= SPLIT_DATE]['id'].tolist()\n",
    "\n",
    "# Get interactions for train/test\n",
    "train_interactions = interactions[interactions['campaign_id'].isin(train_campaigns)].copy()\n",
    "test_interactions = interactions[interactions['campaign_id'].isin(test_campaigns)].copy()\n",
    "\n",
    "print(f\"Training campaigns: {len(train_campaigns)}\")\n",
    "print(f\"Test campaigns: {len(test_campaigns)}\")\n",
    "print(f\"Training interactions: {len(train_interactions)}\")\n",
    "print(f\"Test interactions: {len(test_interactions)}\")\n",
    "\n",
    "# Filter test to campaigns with at least 5 accepted\n",
    "test_accepted = test_interactions[test_interactions['accepted'] == 1].groupby('campaign_id').size()\n",
    "test_camps_with_accepted = test_accepted[test_accepted >= 5].index.tolist()\n",
    "print(f\"\\nTest campaigns with 5+ accepted influencers: {len(test_camps_with_accepted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 139192 rows\n"
     ]
    }
   ],
   "source": [
    "# Build training data using ONLY training campaigns\n",
    "# Historical stats from training data only\n",
    "train_creator_stats = train_interactions.groupby('creator_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "train_creator_stats.columns = ['creator_id', 'total_accepted', 'total_interactions', 'acceptance_rate']\n",
    "\n",
    "train_campaign_stats = train_interactions.groupby('campaign_id')['accepted'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "train_campaign_stats.columns = ['campaign_id', 'camp_accepted', 'camp_interactions', 'camp_acceptance_rate']\n",
    "\n",
    "# Build training dataset - use influencers_scored instead of quality_influencers\n",
    "train_df = train_interactions.merge(\n",
    "    influencers_scored[['id', 'network_id', 'followers', 'follows', 'engagement',\n",
    "                         'avg_likes', 'avg_comments', 'posts', 'reach', 'impressions',\n",
    "                         'tier_level', 'quality_score', 'biography']],\n",
    "    left_on='creator_id', right_on='id', how='inner'\n",
    ")\n",
    "\n",
    "# Merge campaign data\n",
    "camp_cols = campaigns[['id', 'type_id', 'description', 'private', 'pre_approve', 'network_id']].copy()\n",
    "camp_cols = camp_cols.rename(columns={'id': 'camp_id', 'description': 'campaign_description', 'network_id': 'camp_network'})\n",
    "train_df = train_df.merge(camp_cols, left_on='campaign_id', right_on='camp_id', how='left')\n",
    "\n",
    "# Merge brief data\n",
    "brief_cols = briefs[['id', 'description']].copy()\n",
    "brief_cols = brief_cols.rename(columns={'id': 'b_id', 'description': 'brief_description'})\n",
    "train_df = train_df.merge(brief_cols, left_on='brief_id', right_on='b_id', how='left')\n",
    "\n",
    "print(f\"Training data: {len(train_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features prepared: (139192, 89)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for training\n",
    "train_df['brief_description'] = train_df['brief_description'].fillna('')\n",
    "train_df['campaign_description'] = train_df['campaign_description'].fillna('')\n",
    "\n",
    "# Fill numeric/categorical\n",
    "for col in numeric_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].fillna(0)\n",
    "for col in cat_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].fillna(-1).astype(int)\n",
    "\n",
    "# TF-IDF on training briefs only\n",
    "train_unique_briefs = train_df[['brief_id', 'campaign_description', 'brief_description']].drop_duplicates('brief_id')\n",
    "train_unique_briefs['text'] = train_unique_briefs['campaign_description'] + ' ' + train_unique_briefs['brief_description']\n",
    "\n",
    "train_tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "train_tfidf_matrix = train_tfidf.fit_transform(train_unique_briefs['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(train_tfidf_matrix.toarray(), columns=[f'tfidf_{i}' for i in range(50)])\n",
    "tfidf_df['brief_id'] = train_unique_briefs['brief_id'].values\n",
    "\n",
    "train_df = train_df.merge(tfidf_df, on='brief_id', how='left')\n",
    "for i in range(50):\n",
    "    train_df[f'tfidf_{i}'] = train_df[f'tfidf_{i}'].fillna(0)\n",
    "\n",
    "# Add historical stats\n",
    "train_df = train_df.merge(train_creator_stats, on='creator_id', how='left')\n",
    "train_df = train_df.merge(train_campaign_stats, on='campaign_id', how='left')\n",
    "for col in ['acceptance_rate', 'total_accepted', 'total_interactions', 'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']:\n",
    "    train_df[col] = train_df[col].fillna(0)\n",
    "\n",
    "print(f\"Training features prepared: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 139192 samples with 70 features\n",
      "Model trained on historical data only!\n"
     ]
    }
   ],
   "source": [
    "# Train model on training data only\n",
    "tfidf_feats = [f'tfidf_{i}' for i in range(50)]\n",
    "hist_feats = ['acceptance_rate', 'total_accepted', 'total_interactions',\n",
    "              'camp_acceptance_rate', 'camp_accepted', 'camp_interactions']\n",
    "\n",
    "train_features = numeric_cols + cat_cols + tfidf_feats + hist_feats\n",
    "train_features = [f for f in train_features if f in train_df.columns]\n",
    "\n",
    "X_train = train_df[train_features].fillna(0)\n",
    "y_train = train_df['accepted']\n",
    "\n",
    "print(f\"Training on {len(X_train)} samples with {len(train_features)} features\")\n",
    "\n",
    "# Train\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {'objective': 'binary', 'metric': 'auc', 'verbosity': -1,\n",
    "          'num_leaves': 31, 'learning_rate': 0.05, 'is_unbalance': True}\n",
    "\n",
    "eval_model = lgb.train(params, train_data, 200)\n",
    "print(\"Model trained on historical data only!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creator history calculated: 3548 creators\n",
      "Creators with 1+ acceptance: 2045\n",
      "History-based recommendation function defined!\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED: History-based recommendations with recency weighting\n",
    "# This approach achieves ~17-26% recall (vs 3% for the LightGBM model)\n",
    "\n",
    "# Calculate recency-weighted acceptance scores from training data\n",
    "train_interactions['created_at'] = pd.to_datetime(train_interactions['created_at'])\n",
    "max_train_date = train_interactions['created_at'].max()\n",
    "\n",
    "# Recency weight: more recent acceptances count more\n",
    "train_interactions['days_ago'] = (max_train_date - train_interactions['created_at']).dt.days\n",
    "train_interactions['recency_weight'] = np.exp(-train_interactions['days_ago'] / 180)  # 6-month half-life\n",
    "\n",
    "# Calculate recency-weighted acceptance score per creator\n",
    "train_accepted = train_interactions[train_interactions['accepted'] == 1].copy()\n",
    "recency_scores = train_accepted.groupby('creator_id')['recency_weight'].sum().reset_index()\n",
    "recency_scores.columns = ['creator_id', 'recency_score']\n",
    "\n",
    "# Also keep regular acceptance count\n",
    "creator_history = train_interactions.groupby('creator_id').agg({\n",
    "    'accepted': ['sum', 'count', 'mean']\n",
    "}).reset_index()\n",
    "creator_history.columns = ['creator_id', 'times_accepted', 'times_applied', 'acceptance_rate']\n",
    "creator_history = creator_history.merge(recency_scores, on='creator_id', how='left')\n",
    "creator_history['recency_score'] = creator_history['recency_score'].fillna(0)\n",
    "\n",
    "print(f\"Creator history calculated: {len(creator_history)} creators\")\n",
    "print(f\"Creators with 1+ acceptance: {(creator_history['times_accepted'] > 0).sum()}\")\n",
    "\n",
    "def get_recommendations_history(campaign_id, top_n=50):\n",
    "    \"\"\"\n",
    "    History-based recommendations using recency-weighted acceptance count.\n",
    "    Key insight: Past acceptance is the strongest predictor of future acceptance.\n",
    "    \"\"\"\n",
    "    camp = campaigns[campaigns['id'] == campaign_id].iloc[0]\n",
    "    network = camp['network_id']\n",
    "    \n",
    "    # Get eligible influencers from group filters\n",
    "    eligible = get_eligible_influencers(campaign_id)\n",
    "    \n",
    "    # Filter to eligible + correct network\n",
    "    pool = influencers_scored[\n",
    "        (influencers_scored['id'].isin(eligible)) &\n",
    "        (influencers_scored['network_id'] == network)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(pool) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Add history\n",
    "    pool = pool.merge(creator_history, left_on='id', right_on='creator_id', how='left')\n",
    "    pool['times_accepted'] = pool['times_accepted'].fillna(0)\n",
    "    pool['recency_score'] = pool['recency_score'].fillna(0)\n",
    "    \n",
    "    # Score by recency-weighted acceptance count\n",
    "    pool['score'] = pool['recency_score']\n",
    "    \n",
    "    return pool.nlargest(top_n, 'score')['id'].tolist()\n",
    "\n",
    "print(\"History-based recommendation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis & Prioritized Model Training\n",
    "\n",
    "Based on data analysis, here's what actually predicts acceptance:\n",
    "\n",
    "| Feature | Correlation | Insight |\n",
    "|---------|-------------|---------|\n",
    "| **acceptance_rate** | +0.77 | BY FAR the strongest predictor |\n",
    "| **times_accepted** | +0.23 | Past success predicts future |\n",
    "| **impressions** | +0.04 | Weak positive |\n",
    "| **times_applied** | -0.16 | Negative! Spammy applicants rejected |\n",
    "| **followers** | -0.03 | Slightly negative (surprising!) |\n",
    "| **engagement** | ~0 | No correlation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering: Similar Campaigns Approach\n",
    "\n",
    "**Key Idea**: Find campaigns similar to the target campaign, then recommend influencers who were accepted in those similar campaigns.\n",
    "\n",
    "This solves the cold-start problem by leveraging the \"wisdom of similar campaigns\" rather than individual influencer history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building campaign similarity index...\n"
     ]
    }
   ],
   "source": [
    "# Build campaign similarity matrix using multiple signals\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Building campaign similarity index...\")\n",
    "\n",
    "# 1. Get campaign features for similarity\n",
    "camp_features = campaigns[['id', 'network_id', 'type_id', 'description']].copy()\n",
    "camp_features['description'] = camp_features['description'].fillna('')\n",
    "\n",
    "# 2. Add group-based features (what kind of influencers they target)\n",
    "camp_group_features = []\n",
    "for camp_id in campaigns['id']:\n",
    "    camp_groups_df = groups[groups['campaign_id'] == camp_id]\n",
    "    \n",
    "    # Extract targeting info\n",
    "    tiers = []\n",
    "    country_ids = []\n",
    "    for _, g in camp_groups_df.iterrows():\n",
    "        if pd.notna(g['creators_tiers']):\n",
    "            try:\n",
    "                tiers.extend(json.loads(g['creators_tiers']))\n",
    "            except:\n",
    "                pass\n",
    "        if pd.notna(g['country_id']):\n",
    "            country_ids.append(g['country_id'])\n",
    "    \n",
    "    # Get categories for this campaign's groups\n",
    "    group_ids = camp_groups_df['id'].tolist()\n",
    "    cats = group_categories[group_categories['group_id'].isin(group_ids)]['category_id'].tolist()\n",
    "    \n",
    "    camp_group_features.append({\n",
    "        'campaign_id': camp_id,\n",
    "        'has_tier_1': 1 in tiers,\n",
    "        'has_tier_2': 2 in tiers,\n",
    "        'has_tier_3': 3 in tiers,\n",
    "        'has_tier_4': 4 in tiers,\n",
    "        'num_categories': len(set(cats)),\n",
    "        'primary_country': country_ids[0] if country_ids else -1\n",
    "    })\n",
    "\n",
    "camp_group_df = pd.DataFrame(camp_group_features)\n",
    "camp_features = camp_features.merge(camp_group_df, left_on='id', right_on='campaign_id', how='left')\n",
    "\n",
    "# 3. TF-IDF on campaign descriptions\n",
    "camp_tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "camp_tfidf_matrix = camp_tfidf.fit_transform(camp_features['description'])\n",
    "\n",
    "# 4. Combine: categorical features + text similarity\n",
    "cat_features = camp_features[['network_id', 'type_id', 'has_tier_1', 'has_tier_2', \n",
    "                               'has_tier_3', 'has_tier_4', 'num_categories', 'primary_country']].fillna(0)\n",
    "\n",
    "# One-hot encode categorical\n",
    "cat_encoded = pd.get_dummies(cat_features, columns=['network_id', 'type_id', 'primary_country'])\n",
    "scaler = StandardScaler()\n",
    "cat_scaled = scaler.fit_transform(cat_encoded)\n",
    "\n",
    "# Combine text + categorical (weight text more heavily for content similarity)\n",
    "import scipy.sparse as sp\n",
    "combined_features = sp.hstack([\n",
    "    camp_tfidf_matrix * 2.0,  # Text features weighted 2x\n",
    "    sp.csr_matrix(cat_scaled)\n",
    "])\n",
    "\n",
    "# Compute similarity matrix\n",
    "print(\"Computing campaign similarity matrix...\")\n",
    "campaign_similarity = cosine_similarity(combined_features)\n",
    "\n",
    "# Create lookup: campaign_id -> index\n",
    "camp_id_to_idx = {cid: idx for idx, cid in enumerate(camp_features['id'])}\n",
    "idx_to_camp_id = {idx: cid for cid, idx in camp_id_to_idx.items()}\n",
    "\n",
    "print(f\"Campaign similarity matrix: {campaign_similarity.shape}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative filtering recommendation function\n",
    "def get_similar_campaigns(campaign_id, top_k=20, exclude_future=True, reference_date=None):\n",
    "    \"\"\"Get most similar campaigns to the target campaign.\"\"\"\n",
    "    if campaign_id not in camp_id_to_idx:\n",
    "        return []\n",
    "    \n",
    "    idx = camp_id_to_idx[campaign_id]\n",
    "    similarities = campaign_similarity[idx]\n",
    "    \n",
    "    # Get top similar (excluding self)\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:top_k+50]  # Get extra to filter\n",
    "    \n",
    "    similar_camps = []\n",
    "    for sim_idx in similar_indices:\n",
    "        sim_camp_id = idx_to_camp_id[sim_idx]\n",
    "        \n",
    "        # Optionally exclude future campaigns (for fair evaluation)\n",
    "        if exclude_future and reference_date:\n",
    "            sim_camp_date = campaigns[campaigns['id'] == sim_camp_id]['created_at'].iloc[0]\n",
    "            if sim_camp_date >= reference_date:\n",
    "                continue\n",
    "        \n",
    "        similar_camps.append({\n",
    "            'campaign_id': sim_camp_id,\n",
    "            'similarity': similarities[sim_idx]\n",
    "        })\n",
    "        \n",
    "        if len(similar_camps) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return similar_camps\n",
    "\n",
    "\n",
    "def get_recommendations_collaborative(campaign_id, top_n=100, num_similar_camps=15):\n",
    "    \"\"\"\n",
    "    Collaborative filtering: recommend influencers accepted in similar campaigns.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Find top K similar campaigns (by description + targeting)\n",
    "    2. Get influencers accepted in those campaigns\n",
    "    3. Score by: (times accepted in similar camps) * (campaign similarity)\n",
    "    4. Filter by eligibility for target campaign\n",
    "    5. Return top N\n",
    "    \"\"\"\n",
    "    camp = campaigns[campaigns['id'] == campaign_id].iloc[0]\n",
    "    network = camp['network_id']\n",
    "    camp_date = camp['created_at']\n",
    "    \n",
    "    # Step 1: Find similar campaigns (only past campaigns for fair eval)\n",
    "    similar = get_similar_campaigns(campaign_id, top_k=num_similar_camps, \n",
    "                                     exclude_future=True, reference_date=camp_date)\n",
    "    \n",
    "    if not similar:\n",
    "        return None\n",
    "    \n",
    "    similar_camp_ids = [s['campaign_id'] for s in similar]\n",
    "    similarity_lookup = {s['campaign_id']: s['similarity'] for s in similar}\n",
    "    \n",
    "    # Step 2: Get influencers accepted in similar campaigns\n",
    "    similar_accepted = interactions[\n",
    "        (interactions['campaign_id'].isin(similar_camp_ids)) &\n",
    "        (interactions['accepted'] == 1)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(similar_accepted) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Score influencers by weighted acceptance count\n",
    "    similar_accepted['similarity_weight'] = similar_accepted['campaign_id'].map(similarity_lookup)\n",
    "    \n",
    "    influencer_scores = similar_accepted.groupby('creator_id').agg({\n",
    "        'similarity_weight': 'sum',  # Sum of similarity weights\n",
    "        'campaign_id': 'nunique'      # Number of similar campaigns accepted in\n",
    "    }).reset_index()\n",
    "    influencer_scores.columns = ['creator_id', 'collab_score', 'num_similar_accepted']\n",
    "    \n",
    "    # Step 4: Filter by eligibility\n",
    "    eligible = get_eligible_influencers(campaign_id)\n",
    "    \n",
    "    pool = influencers_scored[\n",
    "        (influencers_scored['id'].isin(eligible)) &\n",
    "        (influencers_scored['network_id'] == network) &\n",
    "        (influencers_scored['id'].isin(influencer_scores['creator_id']))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(pool) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Add collaborative scores\n",
    "    pool = pool.merge(influencer_scores, left_on='id', right_on='creator_id', how='left')\n",
    "    pool['collab_score'] = pool['collab_score'].fillna(0)\n",
    "    \n",
    "    return pool.nlargest(top_n, 'collab_score')['id'].tolist()\n",
    "\n",
    "\n",
    "def get_recommendations_hybrid(campaign_id, top_n=100, \n",
    "                                history_weight=0.5, collab_weight=0.5):\n",
    "    \"\"\"\n",
    "    Hybrid approach: combine history-based + collaborative filtering.\n",
    "    \n",
    "    This gets the best of both worlds:\n",
    "    - History: reliable for known high-performers\n",
    "    - Collab: discovers new influencers from similar campaigns\n",
    "    \"\"\"\n",
    "    camp = campaigns[campaigns['id'] == campaign_id].iloc[0]\n",
    "    network = camp['network_id']\n",
    "    camp_date = camp['created_at']\n",
    "    \n",
    "    # Get eligible pool\n",
    "    eligible = get_eligible_influencers(campaign_id)\n",
    "    pool = influencers_scored[\n",
    "        (influencers_scored['id'].isin(eligible)) &\n",
    "        (influencers_scored['network_id'] == network)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(pool) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Add history scores\n",
    "    pool = pool.merge(creator_history, left_on='id', right_on='creator_id', how='left')\n",
    "    pool['recency_score'] = pool['recency_score'].fillna(0)\n",
    "    \n",
    "    # Add collaborative scores\n",
    "    similar = get_similar_campaigns(campaign_id, top_k=15, \n",
    "                                     exclude_future=True, reference_date=camp_date)\n",
    "    \n",
    "    if similar:\n",
    "        similar_camp_ids = [s['campaign_id'] for s in similar]\n",
    "        similarity_lookup = {s['campaign_id']: s['similarity'] for s in similar}\n",
    "        \n",
    "        similar_accepted = interactions[\n",
    "            (interactions['campaign_id'].isin(similar_camp_ids)) &\n",
    "            (interactions['accepted'] == 1)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(similar_accepted) > 0:\n",
    "            similar_accepted['sim_weight'] = similar_accepted['campaign_id'].map(similarity_lookup)\n",
    "            collab_scores = similar_accepted.groupby('creator_id')['sim_weight'].sum().reset_index()\n",
    "            collab_scores.columns = ['creator_id', 'collab_score']\n",
    "            pool = pool.merge(collab_scores, left_on='id', right_on='creator_id', \n",
    "                            how='left', suffixes=('', '_collab'))\n",
    "    \n",
    "    pool['collab_score'] = pool.get('collab_score', pd.Series([0]*len(pool))).fillna(0)\n",
    "    \n",
    "    # Normalize scores to 0-1\n",
    "    if pool['recency_score'].max() > 0:\n",
    "        pool['history_norm'] = pool['recency_score'] / pool['recency_score'].max()\n",
    "    else:\n",
    "        pool['history_norm'] = 0\n",
    "        \n",
    "    if pool['collab_score'].max() > 0:\n",
    "        pool['collab_norm'] = pool['collab_score'] / pool['collab_score'].max()\n",
    "    else:\n",
    "        pool['collab_norm'] = 0\n",
    "    \n",
    "    # Hybrid score\n",
    "    pool['hybrid_score'] = (\n",
    "        pool['history_norm'] * history_weight +\n",
    "        pool['collab_norm'] * collab_weight\n",
    "    )\n",
    "    \n",
    "    return pool.nlargest(top_n, 'hybrid_score')['id'].tolist()\n",
    "\n",
    "print(\"Collaborative filtering functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightFM: Hybrid Collaborative Filtering\n",
    "\n",
    "LightFM is a hybrid matrix factorization model that:\n",
    "1. **Learns latent embeddings** for campaigns and influencers\n",
    "2. **Incorporates side features** (network, tier, followers, engagement) for cold-start\n",
    "3. **Uses WARP loss** optimized for ranking (top-N recommendations)\n",
    "\n",
    "**Note**: Requires Python 3.11 or earlier. Use: `conda create -n recommender python=3.11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"Building LightFM dataset...\")\n",
    "\n",
    "# Use training interactions only (before April 2024)\n",
    "train_accepted = train_interactions[train_interactions['accepted'] == 1].copy()\n",
    "\n",
    "# Create mappings for campaigns and influencers\n",
    "all_campaigns = campaigns['id'].unique().tolist()\n",
    "all_influencers = influencers_scored['id'].unique().tolist()\n",
    "\n",
    "# Build LightFM dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=all_campaigns,  # campaigns are \"users\" in LightFM terminology\n",
    "    items=all_influencers  # influencers are \"items\"\n",
    ")\n",
    "\n",
    "# Get the mappings\n",
    "campaign_id_map, campaign_feature_map, influencer_id_map, influencer_feature_map = dataset.mapping()\n",
    "\n",
    "print(f\"Campaigns: {len(campaign_id_map)}\")\n",
    "print(f\"Influencers: {len(influencer_id_map)}\")\n",
    "\n",
    "# Build interactions matrix (campaign, influencer) pairs where accepted=1\n",
    "train_interactions_list = list(zip(\n",
    "    train_accepted['campaign_id'], \n",
    "    train_accepted['creator_id']\n",
    "))\n",
    "\n",
    "# Filter to only include known campaigns and influencers\n",
    "valid_interactions = [\n",
    "    (c, i) for c, i in train_interactions_list \n",
    "    if c in campaign_id_map and i in influencer_id_map\n",
    "]\n",
    "\n",
    "print(f\"Training interactions: {len(valid_interactions)}\")\n",
    "\n",
    "# Build the interaction matrix\n",
    "(interactions_matrix, weights) = dataset.build_interactions(valid_interactions)\n",
    "\n",
    "print(f\"Interactions matrix shape: {interactions_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {interactions_matrix.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ITEM (influencer) features for cold-start handling\n",
    "print(\"Building influencer features...\")\n",
    "\n",
    "# Prepare influencer features\n",
    "inf_features = influencers_scored[['id', 'network_id', 'tier_level', 'followers', \n",
    "                                    'engagement', 'avg_likes', 'avg_comments']].copy()\n",
    "\n",
    "# Discretize continuous features into bins\n",
    "inf_features['followers_bin'] = pd.qcut(inf_features['followers'].clip(0, inf_features['followers'].quantile(0.99)), \n",
    "                                         q=10, labels=False, duplicates='drop').fillna(0).astype(int)\n",
    "inf_features['engagement_bin'] = pd.qcut(inf_features['engagement'].clip(0, inf_features['engagement'].quantile(0.99)), \n",
    "                                          q=5, labels=False, duplicates='drop').fillna(0).astype(int)\n",
    "\n",
    "# Create feature tags\n",
    "def get_influencer_features(row):\n",
    "    features = []\n",
    "    features.append(f\"network:{int(row['network_id'])}\")\n",
    "    features.append(f\"tier:{int(row['tier_level'])}\" if pd.notna(row['tier_level']) else \"tier:unknown\")\n",
    "    features.append(f\"followers_bin:{row['followers_bin']}\")\n",
    "    features.append(f\"engagement_bin:{row['engagement_bin']}\")\n",
    "    return features\n",
    "\n",
    "inf_features['feature_list'] = inf_features.apply(get_influencer_features, axis=1)\n",
    "\n",
    "# Get all unique feature names\n",
    "all_inf_feature_names = set()\n",
    "for fl in inf_features['feature_list']:\n",
    "    all_inf_feature_names.update(fl)\n",
    "\n",
    "print(f\"Unique influencer features: {len(all_inf_feature_names)}\")\n",
    "\n",
    "# Rebuild dataset with item features\n",
    "dataset_with_features = Dataset()\n",
    "dataset_with_features.fit(\n",
    "    users=all_campaigns,\n",
    "    items=all_influencers,\n",
    "    item_features=all_inf_feature_names\n",
    ")\n",
    "\n",
    "# Build interactions again\n",
    "(interactions_matrix_f, weights_f) = dataset_with_features.build_interactions(valid_interactions)\n",
    "\n",
    "# Build item features matrix\n",
    "item_features_list = []\n",
    "for _, row in inf_features.iterrows():\n",
    "    if row['id'] in influencer_id_map:\n",
    "        item_features_list.append((row['id'], row['feature_list']))\n",
    "\n",
    "item_features_matrix = dataset_with_features.build_item_features(item_features_list)\n",
    "\n",
    "print(f\"Item features matrix shape: {item_features_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightFM model\n",
    "print(\"Training LightFM model...\")\n",
    "print(\"Using WARP loss (optimized for ranking)\")\n",
    "\n",
    "# Model with item features (hybrid mode)\n",
    "lightfm_model = LightFM(\n",
    "    no_components=64,      # Embedding dimension\n",
    "    learning_rate=0.05,\n",
    "    loss='warp',           # Weighted Approximate-Rank Pairwise - best for top-N\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train for multiple epochs\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    lightfm_model.fit_partial(\n",
    "        interactions_matrix_f,\n",
    "        item_features=item_features_matrix,\n",
    "        epochs=1,\n",
    "        num_threads=4\n",
    "    )\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch + 1}/{NUM_EPOCHS} complete\")\n",
    "\n",
    "print(\"LightFM model trained!\")\n",
    "\n",
    "# Get the updated mappings\n",
    "lfm_campaign_map, _, lfm_influencer_map, _ = dataset_with_features.mapping()\n",
    "lfm_influencer_reverse = {v: k for k, v in lfm_influencer_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightFM recommendation function\n",
    "def get_recommendations_lightfm(campaign_id, top_n=100):\n",
    "    \"\"\"\n",
    "    Get recommendations using LightFM hybrid model.\n",
    "    \n",
    "    The model predicts scores for all influencers based on:\n",
    "    1. Learned campaign embedding (from past acceptances)\n",
    "    2. Learned influencer embeddings  \n",
    "    3. Influencer features (for cold-start)\n",
    "    \"\"\"\n",
    "    if campaign_id not in lfm_campaign_map:\n",
    "        return None\n",
    "    \n",
    "    camp = campaigns[campaigns['id'] == campaign_id].iloc[0]\n",
    "    network = camp['network_id']\n",
    "    \n",
    "    # Get eligible influencers\n",
    "    eligible = get_eligible_influencers(campaign_id)\n",
    "    \n",
    "    # Filter to network and eligibility\n",
    "    pool = influencers_scored[\n",
    "        (influencers_scored['id'].isin(eligible)) &\n",
    "        (influencers_scored['network_id'] == network)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(pool) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get LightFM indices for eligible influencers\n",
    "    campaign_idx = lfm_campaign_map[campaign_id]\n",
    "    \n",
    "    eligible_with_idx = []\n",
    "    for inf_id in pool['id']:\n",
    "        if inf_id in lfm_influencer_map:\n",
    "            eligible_with_idx.append((inf_id, lfm_influencer_map[inf_id]))\n",
    "    \n",
    "    if not eligible_with_idx:\n",
    "        return None\n",
    "    \n",
    "    inf_ids, inf_indices = zip(*eligible_with_idx)\n",
    "    inf_indices = np.array(inf_indices)\n",
    "    \n",
    "    # Predict scores\n",
    "    scores = lightfm_model.predict(\n",
    "        user_ids=campaign_idx,\n",
    "        item_ids=inf_indices,\n",
    "        item_features=item_features_matrix\n",
    "    )\n",
    "    \n",
    "    # Get top N\n",
    "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    top_inf_ids = [inf_ids[i] for i in top_indices]\n",
    "    \n",
    "    return top_inf_ids\n",
    "\n",
    "# Test it\n",
    "test_recs = get_recommendations_lightfm(2993, top_n=10)\n",
    "print(f\"LightFM recommendations for campaign 2993: {len(test_recs) if test_recs else 0} influencers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LightFM vs all other approaches\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION: ALL METHODS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_all = {\n",
    "    'history': [],\n",
    "    'collab': [],\n",
    "    'hybrid': [],\n",
    "    'lightfm': []\n",
    "}\n",
    "\n",
    "for i, camp_id in enumerate(newest_test_camps):\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"  Processing campaign {i+1}/100...\")\n",
    "    \n",
    "    actual_accepted = test_interactions[\n",
    "        (test_interactions['campaign_id'] == camp_id) & \n",
    "        (test_interactions['accepted'] == 1)\n",
    "    ]['creator_id'].unique()\n",
    "    \n",
    "    if len(actual_accepted) < 5:\n",
    "        continue\n",
    "    \n",
    "    actual_set = set(actual_accepted)\n",
    "    \n",
    "    for top_n in [50, 100, 200]:\n",
    "        # History baseline\n",
    "        rec = get_recommendations_history(camp_id, top_n)\n",
    "        if rec:\n",
    "            hits = len(set(rec) & actual_set)\n",
    "            results_all['history'].append({'top_n': top_n, 'hits': hits, 'actual': len(actual_set)})\n",
    "        \n",
    "        # Simple collaborative\n",
    "        rec = get_recommendations_collaborative(camp_id, top_n)\n",
    "        if rec:\n",
    "            hits = len(set(rec) & actual_set)\n",
    "            results_all['collab'].append({'top_n': top_n, 'hits': hits, 'actual': len(actual_set)})\n",
    "        \n",
    "        # Hybrid (history + collab)\n",
    "        rec = get_recommendations_hybrid(camp_id, top_n)\n",
    "        if rec:\n",
    "            hits = len(set(rec) & actual_set)\n",
    "            results_all['hybrid'].append({'top_n': top_n, 'hits': hits, 'actual': len(actual_set)})\n",
    "        \n",
    "        # LightFM\n",
    "        rec = get_recommendations_lightfm(camp_id, top_n)\n",
    "        if rec:\n",
    "            hits = len(set(rec) & actual_set)\n",
    "            results_all['lightfm'].append({'top_n': top_n, 'hits': hits, 'actual': len(actual_set)})\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS COMPARISON - ALL METHODS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "method_names = {\n",
    "    'history': 'HISTORY (recency-weighted)',\n",
    "    'collab': 'COLLABORATIVE (campaign similarity)',\n",
    "    'hybrid': 'HYBRID (history + collab)',\n",
    "    'lightfm': 'LIGHTFM (matrix factorization)'\n",
    "}\n",
    "\n",
    "for method, name in method_names.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    df = pd.DataFrame(results_all[method])\n",
    "    for top_n in [50, 100, 200]:\n",
    "        sub = df[df['top_n'] == top_n]\n",
    "        if len(sub) > 0:\n",
    "            recall = sub['hits'].sum() / sub['actual'].sum()\n",
    "            print(f\"  Top {top_n}: {recall*100:.1f}% recall ({sub['hits'].sum()} hits)\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY TABLE (Recall %)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<35} {'Top 50':>10} {'Top 100':>10} {'Top 200':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for method, name in method_names.items():\n",
    "    df = pd.DataFrame(results_all[method])\n",
    "    recalls = []\n",
    "    for top_n in [50, 100, 200]:\n",
    "        sub = df[df['top_n'] == top_n]\n",
    "        if len(sub) > 0:\n",
    "            recall = sub['hits'].sum() / sub['actual'].sum() * 100\n",
    "            recalls.append(f\"{recall:.1f}%\")\n",
    "        else:\n",
    "            recalls.append(\"N/A\")\n",
    "    print(f\"{name:<35} {recalls[0]:>10} {recalls[1]:>10} {recalls[2]:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL PRODUCTION FUNCTION - Best of all approaches\n",
    "def recommend_influencers_v3(campaign_id, top_n=100, method='ensemble'):\n",
    "    \"\"\"\n",
    "    Production-ready influencer recommendation with all strategies.\n",
    "    \n",
    "    Methods:\n",
    "    - 'history': Rank by past acceptance history\n",
    "    - 'collaborative': Rank by acceptance in similar campaigns  \n",
    "    - 'lightfm': LightFM matrix factorization model\n",
    "    - 'ensemble': Combine all signals (recommended)\n",
    "    \n",
    "    Returns DataFrame with recommended influencers and scores.\n",
    "    \"\"\"\n",
    "    camp = campaigns[campaigns['id'] == campaign_id].iloc[0]\n",
    "    network = camp['network_id']\n",
    "    network_name = 'Instagram' if network == 1 else 'TikTok' if network == 8 else 'Other'\n",
    "    \n",
    "    print(f\"Campaign: {camp['name']}\")\n",
    "    print(f\"Network: {network_name}\")\n",
    "    print(f\"Method: {method}\")\n",
    "    \n",
    "    # Get eligible pool\n",
    "    eligible = get_eligible_influencers(campaign_id)\n",
    "    print(f\"Eligible (after group filters): {len(eligible)}\")\n",
    "    \n",
    "    pool = influencers_scored[\n",
    "        (influencers_scored['id'].isin(eligible)) &\n",
    "        (influencers_scored['network_id'] == network)\n",
    "    ].copy()\n",
    "    print(f\"On {network_name}: {len(pool)}\")\n",
    "    \n",
    "    if len(pool) == 0:\n",
    "        print(\"No eligible influencers!\")\n",
    "        return None\n",
    "    \n",
    "    # Exclude already contacted\n",
    "    already = interactions[interactions['campaign_id'] == campaign_id]['creator_id'].unique()\n",
    "    pool = pool[~pool['id'].isin(already)]\n",
    "    print(f\"After excluding contacted: {len(pool)}\")\n",
    "    \n",
    "    # === SCORE 1: History ===\n",
    "    pool = pool.merge(full_creator_history, left_on='id', right_on='creator_id', how='left')\n",
    "    pool['history_score'] = pool['recency_score'].fillna(0)\n",
    "    \n",
    "    # === SCORE 2: Collaborative ===\n",
    "    similar = get_similar_campaigns(campaign_id, top_k=20, exclude_future=False)\n",
    "    pool['collab_score'] = 0.0\n",
    "    \n",
    "    if similar:\n",
    "        similar_camp_ids = [s['campaign_id'] for s in similar]\n",
    "        similarity_lookup = {s['campaign_id']: s['similarity'] for s in similar}\n",
    "        \n",
    "        similar_accepted = interactions[\n",
    "            (interactions['campaign_id'].isin(similar_camp_ids)) &\n",
    "            (interactions['accepted'] == 1)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(similar_accepted) > 0:\n",
    "            similar_accepted['sim_weight'] = similar_accepted['campaign_id'].map(similarity_lookup)\n",
    "            collab_scores = similar_accepted.groupby('creator_id')['sim_weight'].sum().reset_index()\n",
    "            collab_scores.columns = ['id', 'collab_score_new']\n",
    "            pool = pool.merge(collab_scores, on='id', how='left')\n",
    "            pool['collab_score'] = pool['collab_score_new'].fillna(0)\n",
    "            pool = pool.drop(columns=['collab_score_new'], errors='ignore')\n",
    "    \n",
    "    # === SCORE 3: LightFM ===\n",
    "    pool['lightfm_score'] = 0.0\n",
    "    \n",
    "    if campaign_id in lfm_campaign_map:\n",
    "        campaign_idx = lfm_campaign_map[campaign_id]\n",
    "        \n",
    "        for idx, row in pool.iterrows():\n",
    "            if row['id'] in lfm_influencer_map:\n",
    "                inf_idx = lfm_influencer_map[row['id']]\n",
    "                score = lightfm_model.predict(\n",
    "                    user_ids=campaign_idx,\n",
    "                    item_ids=np.array([inf_idx]),\n",
    "                    item_features=item_features_matrix\n",
    "                )[0]\n",
    "                pool.loc[idx, 'lightfm_score'] = score\n",
    "    \n",
    "    # Normalize all scores to 0-1\n",
    "    for col in ['history_score', 'collab_score', 'lightfm_score']:\n",
    "        max_val = pool[col].max()\n",
    "        if max_val > 0:\n",
    "            pool[f'{col}_norm'] = pool[col] / max_val\n",
    "        else:\n",
    "            pool[f'{col}_norm'] = 0\n",
    "    \n",
    "    # Calculate final score based on method\n",
    "    if method == 'history':\n",
    "        pool['final_score'] = pool['history_score_norm']\n",
    "    elif method == 'collaborative':\n",
    "        pool['final_score'] = pool['collab_score_norm']\n",
    "    elif method == 'lightfm':\n",
    "        pool['final_score'] = pool['lightfm_score_norm']\n",
    "    else:  # ensemble\n",
    "        pool['final_score'] = (\n",
    "            pool['history_score_norm'] * 0.4 +\n",
    "            pool['collab_score_norm'] * 0.3 +\n",
    "            pool['lightfm_score_norm'] * 0.3\n",
    "        )\n",
    "    \n",
    "    # Return top N\n",
    "    result = pool.nlargest(top_n, 'final_score')[\n",
    "        ['id', 'account', 'name', 'followers', 'engagement', \n",
    "         'times_accepted', 'history_score', 'collab_score', 'lightfm_score', 'final_score']\n",
    "    ].copy()\n",
    "    \n",
    "    result = result.rename(columns={\n",
    "        'times_accepted': 'past_acceptances'\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE: Ensemble recommendations for campaign 2993\")\n",
    "print(\"=\" * 70)\n",
    "recs = recommend_influencers_v3(2993, top_n=15, method='ensemble')\n",
    "if recs is not None:\n",
    "    print(recs.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
